{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
      "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
      "       'pH', 'sulphates', 'alcohol', 'quality', 'Id'],\n",
      "      dtype='object')\n",
      "[[-0.52157961  0.93933222 -1.36502663 ... -0.57365783 -0.96338181\n",
      "  -1.73561799]\n",
      " [-0.29259344  1.94181282 -1.36502663 ...  0.1308811  -0.59360107\n",
      "  -1.73346186]\n",
      " [-0.29259344  1.27349242 -1.16156762 ... -0.04525363 -0.59360107\n",
      "  -1.73130573]\n",
      " ...\n",
      " [-1.20853813  0.38239855 -0.9581086  ... -0.45623467  0.05351522\n",
      "   1.70125196]\n",
      " [-1.38027776  0.10393172 -0.8563791  ...  0.60057372  0.70063152\n",
      "   1.70340809]\n",
      " [-1.38027776  0.6330187  -0.75464959 ...  0.30701583 -0.22382033\n",
      "   1.70772035]]\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'elasticnet', 'l2'} or None. Got 'none' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m     29\u001b[0m clf_no_reg \u001b[38;5;241m=\u001b[39m LogisticRegression(penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mclf_no_reg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m## 4. Plot the coefficients\u001b[39;00m\n\u001b[0;32m     33\u001b[0m predictors \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[1;32mc:\\Programowanie\\.vscode\\.conda\\Lib\\site-packages\\sklearn\\base.py:1466\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1461\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1462\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1463\u001b[0m )\n\u001b[0;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1466\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Programowanie\\.vscode\\.conda\\Lib\\site-packages\\sklearn\\base.py:666\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    659\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \n\u001b[0;32m    661\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 666\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Programowanie\\.vscode\\.conda\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'elasticnet', 'l2'} or None. Got 'none' instead."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(f\"C:\\Programowanie\\codecademy_projects\\data_sets_codecademy\\WineQT.csv\")\n",
    "print(df.columns)\n",
    "y = df['quality']\n",
    "features = df.drop(columns = ['quality'])\n",
    "\n",
    "\n",
    "## 1. Data transformation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(features)\n",
    "\n",
    "X = scaler.transform(features)\n",
    "\n",
    "print(X)\n",
    "\n",
    "\n",
    "## 2. Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)\n",
    "## 3. Fit a logistic regression classifier without regularization\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_no_reg = LogisticRegression(penalty='none')\n",
    "\n",
    "clf_no_reg.fit(X, y)\n",
    "## 4. Plot the coefficients\n",
    "predictors = features.columns\n",
    "coefficients = clf_no_reg.coef_.ravel()\n",
    "coef = pd.Series(coefficients,predictors).sort_values()\n",
    "coef.plot(kind='bar', title = 'Coefficients (no regularization)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "## 5. Training and test performance\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred_train = clf_no_reg.predict(X_train)\n",
    "y_pred_test = clf_no_reg.predict(X_test)\n",
    "\n",
    "print('Training score', f1_score(y_train ,y_pred_train))\n",
    "print('Test score', f1_score(y_test, y_pred_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 6. Default Implementation (L2-regularized!)\n",
    "\n",
    "clf_default = LogisticRegression()\n",
    "clf_default.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "## 7. Ridge Scores\n",
    "y_pred_train_d = clf_default.predict(X_train)\n",
    "y_pred_test_d = clf_default.predict(X_test)\n",
    "\n",
    "print('Training score_d', f1_score(y_train ,y_pred_train_d))\n",
    "print('Test score_d', f1_score(y_test, y_pred_test_d))\n",
    "\n",
    "## 8. Coarse-grained hyperparameter tuning\n",
    "training_array = []\n",
    "test_array = []\n",
    "C_array = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "for i in C_array:\n",
    "  model = LogisticRegression(C = i)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred_train_model = model.predict(X_train)\n",
    "  y_pred_test_model = model.predict(X_test)\n",
    "  f1_train = f1_score(y_train, y_pred_train_model)\n",
    "  f1_test = f1_score(y_test, y_pred_test_model)\n",
    "  training_array.append(f1_train)\n",
    "  test_array.append(f1_test)\n",
    "\n",
    "print(test_array)\n",
    "\n",
    "\n",
    "## 9. Plot training and test scores as a function of C\n",
    "plt.plot(C_array,training_array)\n",
    "plt.plot(C_array,test_array)\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "## 10. Making a parameter grid for GridSearchCV\n",
    "C_array_log = np.logspace(-4, -2, 100)\n",
    "C = {'C': C_array_log}\n",
    "\n",
    "\n",
    "## 11. Implementing GridSearchCV with l2 penalty\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model_grid = LogisticRegression()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "  model_grid,\n",
    "  param_grid = C,\n",
    "  scoring = 'f1',\n",
    "  cv = 5)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "## 12. Optimal C value and the score corresponding to it\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "## 13. Validating the \"best classifier\"\n",
    "\n",
    "clf_best = LogisticRegression(C = grid_search.best_params_['C'])\n",
    "clf_best.fit(X_train,y_train)\n",
    "y_pred_best = clf_best.predict(X_test)\n",
    "print(f1_score(y_test,y_pred_best))\n",
    "\n",
    "## 14. Implement L1 hyperparameter tuning with LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "clf_l1 = LogisticRegressionCV(Cs=C_array_log, cv=5, penalty='l1', solver='liblinear', scoring='f1')\n",
    "\n",
    "clf_l1.fit(X, y)\n",
    "## 15. Optimal C value and corresponding coefficients\n",
    "print(clf_l1.coef_)\n",
    "print(clf_l1.C_)\n",
    "\n",
    "\n",
    "## 16. Plotting the tuned L1 coefficients\n",
    "coefficients = clf_l1.coef_.ravel()\n",
    "coef = pd.Series(coefficients,predictors).sort_values()\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "coef.plot(kind='bar', title = 'Coefficients for tuned L1')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ucimlrepo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
